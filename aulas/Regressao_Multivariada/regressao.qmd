---
title: "Análise de Regressão Multivariada"
format: 
  revealjs:
    width: 1600
    height: 900
    footer: ""
    theme: quartomonothemer.scss
    slide-number: c/t
    show-slide-number: all
    preview-links: auto
    self-contained: true
incremental: false
code-link: true
bibliography: references.bib
title-slide-attributes:
    data-background-image: /images/back.jpeg
    data-background-size: cover
    data-background-opacity: "0.3"
execute:
  echo: true
---

## Introdução

<br>

Um pesquisador coletou dados sobre **três variáveis** psicológicas, quatro variáveis acadêmicas (resultados de testes padronizados), 
e o tipo de programa educacional do aluno em 600 estudantes do ensino médio. 


Ele está interessado em descobrir como o **conjunto de variáveis psicológicas** está relacionado com as variáveis acadêmicas e o tipo de programa que o aluno está inserido.


## Introdução

<br>

Um médico coletou dados sobre o nível de colesterol, pressão arterial e peso. Ele também coletou dados sobre os hábitos alimentares
dos pacientes (por exemplo, o quanto de carne vermelha, peixe, produtos lácteos e chocolate são consumidos por semana). 

Ele quer investigar a relação entre as **três medidas de saúde** e hábitos alimentares de seus pacientes.


## Introdução

<br>

- **Regressão Linear Simples:** Temos **uma variável dependente** $Y$ e **uma variável independente** $X$.

\vspace{0.4cm}
 
- **Regressão Linear Múltipla:** Temos **uma variável dependente** $Y$ e **várias variáveis independentes** $X_1, X_2, \cdots, X_r$.

\vspace{0.4cm}
 
- **Regressão Linear Múltipla Multivariada:** Temos **várias variáveis dependentes** $Y_1, Y_2, \cdots, Y_p$ e 
**várias variáveis independentes** $X_1, X_2, \cdots, X_r$. Nesse caso, cada variável $Y$ e relacionada com todas as variáveis $X$.


# O caso univariado

## Modelo de regressão linear múltiplo univariado

<br>

Sejam $X_1, X_2, \cdots, X_r$ $r$ variáveis independentes relacionadas à uma variável resposta $Y$.

\vspace{0.3cm}

\item O **modelo de regressão linear múltipla univariado** é dado pela seguinte expressão:

\vspace{0.3cm}

$$\underbrace{Y}_{\text{resposta}} = \underbrace{\beta_0 + \beta_1X_1 + \cdots + \beta_rX_r}_{\text{média; parte estrutural}} + 
\underbrace{\epsilon}_{\text{erro; parte aleatória}}$$


## Modelo de regressão linear múltiplo univariado

<br>


- O modelo é dito linear, pois a parte estrutural é linear nos parâmetros $\beta_j$, $j = 1, \cdots, r$.

\vspace{0.3cm}

- Se dispomos de $n$ observações independentes:

\vspace{0.3cm}

$$Y_{i} = \beta_0 + \beta_1X_{1i} + \cdots + \beta_rX_{ri} + \epsilon_i, \hspace{0.2cm} i = 1, \cdots, n$$

## Modelo de regressão linear múltiplo univariado

<br>

<p align="center"> 
**Suposições**
</p>


- $E(\epsilon_i) = 0$, $\forall i = 1, 2, \cdots, n$

\vspace{0.3cm}

- $\text{Var}(\epsilon_i) = \sigma^2$, $\forall i = 1, 2, \cdots, n$  (homocedasticidade)

\vspace{0.3cm}

- $\text{Cov}(\epsilon_i, \epsilon_k) = 0$, $\forall i \neq k, \hspace{0.3cm} i,k \in \{1, 2, \cdots, n\}$ 



## Modelo de regressão linear múltiplo univariado

<br>


Em notação matricial, temos:

\vspace{0.3cm}

$$\underbrace{\mathbf{y}}_{n \times 1} = \underbrace{\mathbf{X}}_{n \times (r + 1)} \underbrace{\mathbf{\beta}}_{(r + 1) \times 1} 
+ \underbrace{\mathbf{\epsilon}}_{n \times 1} $$


\vspace{0.3cm}

<p align="center"> 
**Suposições**
</p>


- $E(\mathbf{\epsilon}) = \mathbf{0}$



- $\text{Var}(\mathbf{\epsilon}) = \sigma^2 \mathbf{I}_n$


## Modelo de regressão linear múltiplo univariado

<br>

$$
 \mathbf{y} = \left[ \begin{matrix}  y_1 \\  y_2 \\ \vdots \\ y_n \end{matrix} \right] \hspace{1cm} \mathbf{X} = \left[ \begin{matrix}  1 & X_{11} & X_{12} & \cdots & X_{1r} \\  
1 & X_{21} & X_{22} & \cdots & X_{2r} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & X_{n1} & X_{n2} & \cdots & X_{nr} \end{matrix} \right]
$$

$$
 \mathbf{\beta} = \left[ \begin{matrix}  \beta_0 \\  \beta_1 \\ \vdots \\ \beta_r \end{matrix} \right] \hspace{1cm} 
 \mathbf{\epsilon} = \left[ \begin{matrix}  \epsilon_1 \\  \epsilon_2 \\ \vdots \\ \epsilon_n \end{matrix} \right]
$$

## Modelo de regressão linear múltiplo univariado

<br>


<style>
.texto-personalizado {
    color: red;
    font-size: 60px;
    font-weight: bold;
}
</style>




:::: {.columns}

::: {.column width="10%"}
<span style='font-size:150px;'>&#129300;</span> 
:::

::: {.column width="90%"}
<p align="center" class="texto-personalizado">
Observe que ainda não fizemos nenhuma suposição a cerca da distribuição dos erros...
</p>
:::

::::


. . .

<br>

- Para efeito de obter os estimadores de mínimos quadrados, de fato, não é necessária nenhuma suposição sobre a distribuição
da parte aleatória

. . .

- Para fins de **inferência**, essa suposição será necessária...



## Estimadores de mínimos quadrados


<br>

Suponha que a matriz $\mathbf{X}$ seja de posto-completo tal que suas colunas formam um conjunto L.I.


. . .



Neste caso, a matriz $\mathbf{X}^t \mathbf{X}$ é não singular e o estimador de mínimos quadrados do vetor $\mathbf{\beta}$ é dado por


. . .


$$\widehat{\mathbf{\beta}} = (\mathbf{X}^t \mathbf{X})^{-1}\mathbf{X}^t\mathbf{y}$$



## Modelo de regressão linear múltiplo univariado


Os valores ajustados são, então, dados por:

\vspace{0.3cm}

$$\widehat{\mathbf{y}} = \mathbf{X}\widehat{\mathbf{\beta}} = \underbrace{\mathbf{X}(\mathbf{X}^t \mathbf{X})^{-1}\mathbf{X}^t}_{\mathbf{H}}\mathbf{y} = \mathbf{H} \mathbf{y}$$

. . .


e os resíduos


$$\widehat{\mathbf{\epsilon}} = \mathbf{y} - \widehat{\mathbf{y}} = (\underbrace{\mathbf{I} - \mathbf{H}}_{\mathbf{P}} )\mathbf{y}$$

satisfazem (somente quando houver a constante $\beta_0$ no modelo)



$$\mathbf{X}^t\widehat{\mathbf{\epsilon}} = {\mathbf{0}}  \hspace{0.5cm} e \hspace{0.5cm} \widehat{\mathbf{y}}^t\widehat{\mathbf{\epsilon}} = 0 $$

## Modelo de regressão linear múltiplo univariado

- **Observação:**  Temos que $\mathbf{H}$ e $\mathbf{P}$ são matrizes idempotentes ($\mathbf{H} = \mathbf{H}\mathbf{H}$ e $\mathbf{H} = \mathbf{H}^t$).


. . .


 A soma de quadrados de resíduos é

\vspace{0.3cm}

$$\text{SQ Res} = \displaystyle{\sum_{i=1}^n}(y_i - \widehat{y}_i)^2 = \widehat{\mathbf{\epsilon}}^t\widehat{\mathbf{\epsilon}} = \mathbf{y}^t(\mathbf{I} - \mathbf{H})\mathbf{y} = \mathbf{y}^t\mathbf{y} - \mathbf{y}^t \mathbf{X} \widehat{\mathbf{\beta}}$$

. . .


Observe que...


$$\displaystyle{\sum_{i=1}^n} y_i^2 = \mathbf{y}^t \mathbf{y} = (\mathbf{y} - \widehat{\mathbf{y}} + \widehat{\mathbf{y}})^t
(\mathbf{y} - \widehat{\mathbf{y}} + \widehat{\mathbf{y}}) = \widehat{\mathbf{y}}^t\widehat{\mathbf{y}} +
\widehat{\mathbf{\epsilon}}^t \widehat{\mathbf{\epsilon}}$$




## Modelo de regressão linear múltiplo univariado

<br>

Uma vez que a primeira coluna de $\mathbf{X}$ é $\mathbf{1}$, a condição $\mathbf{X}^t\widehat{\mathbf{\epsilon}} = {\mathbf{0}}$ inclui
a exigência $0 = \mathbf{1}^t\widehat{\mathbf{\epsilon}} = \displaystyle{\sum_{j=1}^n} \widehat{\mathbf{\epsilon}}_j = 
\displaystyle{\sum_{j=1}^n} y_j - \displaystyle{\sum_{j=1}^n} \widehat{y}_j$ ou $\bar{y} = \bar{\widehat{y}}$. Subtraindo 
$n\bar{y}^2 = n\bar{\widehat{y}}^2$ de ambos os lados, temos a decomposição básica da soma de quadrados total:

\vspace{0.3cm}

$$\text{SQ Total} = \mathbf{y}^t \mathbf{y} - n\bar{y}^2 = \widehat{\mathbf{y}}^t\widehat{\mathbf{y}} -  n\bar{\widehat{y}}^2 +
\widehat{\mathbf{\epsilon}}^t \widehat{\mathbf{\epsilon}}$$


## Modelo de regressão linear múltiplo univariado

<br>


De forma que, o coeficiente de determinação $R^2$ é dado por:

\vspace{0.3cm}

$$R^2 = 1 - \dfrac{\text{SQ Res}}{\text{SQ Total}} = 1 - \dfrac{\mathbf{y}^t\mathbf{y} -
\mathbf{y}^t \mathbf{X} \widehat{\mathbf{\beta}}}{\widehat{\mathbf{y}}^t\widehat{\mathbf{y}} -  n\bar{\widehat{y}}^2 +
\widehat{\mathbf{\epsilon}}^t \widehat{\mathbf{\epsilon}}}$$

\vspace{0.3cm}

- **Observação:** $R^2$ fornece a proporção da variação total dos $Y_i's$ que é "explicada" pelas variáveis independentes.


## Exemplo: Ajuste de modelo de regressão para dados imobiliários

<br>

Os dados do arquivo **Exemplo_regressao_01.dat** referem-se à avaliação imobiliária de 20 casas de determinado bairro em 
uma cidade. As variáveis envolvidas são:

\vspace{0.5cm}


- $X_1$: Tamanho total da habitação (em milhares de metros quadrados) 
- $X_2$: Valor da avaliação (em milhares de reais) 
- $Y$: Valor da venda (em milhares de reais)



## Exemplo: Ajuste de modelo de regressão para dados imobiliários


```{r, eval=TRUE}
library(pacman)
pacman::p_load(
  tidyverse,
  ggResidpanel
)
```

```{r}
dados = read.table("https://raw.githubusercontent.com/tiagomartin/est022/refs/heads/main/dados/Exemplo_regressao_01.dat", header = TRUE) 
dados %>% str()
```



## Exemplo: Ajuste de modelo de regressão para dados imobiliários


```{r}
attach(dados)

n = length(X1)
n

X0 = rep(1,n)
X0

## matriz X

X = cbind(X0,X1,X2)
X %>% head()
```

## Exemplo: Ajuste de modelo de regressão para dados imobiliários


```{r}

## estimando o vetor beta

betaCh = solve(t(X)%*%X)%*%t(X)%*%Y
betaCh


## valores estimados de Y

YCh = X%*%betaCh
YCh %>% head()
```

## Exemplo: Ajuste de modelo de regressão para dados imobiliários


```{r}
#| output-location: column
## residuo

resCh = Y - YCh
resCh %>% head()
```

```{r}
#| output-location: column
plot(resCh)
abline(h=0, col = "red", lty = 3,lwd = 5)
```

## Exemplo: Ajuste de modelo de regressão para dados imobiliários


```{r}
t(X)%*%resCh

t(YCh)%*%resCh

## Soma de quadrados do residuo

SQRes = t(Y)%*%Y - t(Y)%*%X%*%betaCh
SQRes
```

## Exemplo: Ajuste de modelo de regressão para dados imobiliários

```{r}
## Soma de quadrados total

SQTot = t(YCh)%*%YCh - (n * (mean(YCh)^2)) + t(resCh)%*%resCh
SQTot

## Coeficiente de determinacao R2

R2 = 1 - (SQRes/SQTot)
R2
detach(dados)
```


## Exemplo: Ajuste de modelo de regressão para dados imobiliários

```{r}
model = lm(Y~X1+X2, data = dados)
summary(model)
```

## Exemplo: Ajuste de modelo de regressão para dados imobiliários

```{r}
resid_panel(model)
```



# Generalizando

## Modelos de Regressão Linear Multivariada

<br>

Suponha agora, que a variável resposta é p-variada $\mathbf{Y}$ e que $X_1, X_2, \cdots, X_r$ representam as variáveis 
independentes:

$$Y_1 = \beta_{01} + \beta_{11}X_1 + \cdots + \beta_{r1}X_r + \epsilon_1$$

$$Y_2 = \beta_{02} + \beta_{12}X_1 + \cdots + \beta_{r2}X_r + \epsilon_2$$

$$\vdots \hspace{4cm} \vdots \hspace{4cm} \vdots$$

$$Y_p = \beta_{0p} + \beta_{1p}X_1 + \cdots + \beta_{rp}X_r + \epsilon_p$$


## Modelos de Regressão Linear Multivariada

<br>


- $\mathbf{\epsilon} = [\epsilon_1, \epsilon_2, \cdots, \epsilon_p]^t$ com $E(\mathbf{\epsilon}) = \mathbf{0}$ e $\text{Var}(\mathbf{\epsilon}) = \mathbf{\Sigma}$

\vspace{0.3cm}

&#x1F449; Portanto, os erros associados a diferentes componentes do vetor resposta podem ser correlacionados. 



. . .



<p align="center">
**Notação Matricial**
</p>


 \vspace{0.3cm}

$$\underbrace{\mathbf{Y}}_{n \times p} = \underbrace{\mathbf{X}}_{n \times (r + 1)} \underbrace{\mathcal{B}}_{(r + 1) \times p} 
+ \underbrace{\mathbf{\epsilon}}_{n \times p} $$


## Modelos de Regressão Linear Multivariada



$$
 \mathbf{Y} = \left[ \begin{matrix}  Y_{11} & Y_{12} & \cdots & Y_{1p} \\  Y_{21} & Y_{22} & \cdots & Y_{2p} \\ \vdots & \vdots & \ddots & \vdots
 \\ Y_{n1} & Y_{n2} & \cdots & Y_{np} \end{matrix} \right] = \left[\mathbf{Y}_{(1)} | \mathbf{Y}_{(2)} | \cdots | \mathbf{Y}_{(p)} \right]
$$

$$
 \mathbf{X}_{n \times (r + 1)} = \left[ \begin{matrix}  1 & X_{11} & X_{12} & \cdots & X_{1r} \\  
1 & X_{21} & X_{22} & \cdots & X_{2r} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & X_{n1} & X_{n2} & \cdots & X_{nr} \end{matrix} \right]
$$


## Modelos de Regressão Linear Multivariada



$$
\mathcal{B}_{(r + 1) \times p}= \left[ \begin{matrix}  \beta_{01} & \beta_{02} & \cdots &\beta_{0p} \\  \beta_{11} & \beta_{12} & \cdots & \beta_{1p} \\ 
\vdots & \vdots & \ddots & \vdots \\ \beta_{r1} & \beta_{r2} & \cdots & \beta_{rp} \end{matrix} \right] =  \left[\mathbf{\beta}_{(1)} | 
\mathbf{\beta}_{(2)} | \cdots | \mathbf{\beta}_{(p)} \right] 
$$

$$
  \mathbf{\epsilon} = \left[ \begin{matrix}  \epsilon_{11} & \epsilon_{12} & \cdots & \epsilon_{1p} \\  \epsilon_{21} & \epsilon_{22}
& \cdots & \epsilon_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ \epsilon_{n1} & \epsilon_{n2} & \cdots & \epsilon_{np} \end{matrix} 
\right] = \left[\mathbf{\epsilon}_{(1)} | \mathbf{\epsilon}_{(2)} | \cdots | \mathbf{\epsilon}_{(p)} \right]
$$



## Modelos de Regressão Linear Multivariada


<br>

- **Suposições do modelo:** $\underbrace{\mathbf{Y}}_{n \times p} = \underbrace{\mathbf{X}}_{n \times (r + 1)} \underbrace{\mathcal{B}}_{(r + 1) \times p} + \underbrace{\mathbf{\epsilon}}_{n \times p}$

\vspace{0.3cm}

$$E(\mathbf{\epsilon}_{(i)}) = \mathbf{0}, \text{Cov}(\mathbf{\epsilon}_{(i)}, \mathbf{\epsilon}_{(k)}) = \sigma_{ik} \mathbf{I}_n \hspace{0.5cm} i,k = 1, 2, \cdots, p$$

\vspace{1.8cm}

- As *p* medidas sobre a *i*-ésima observação têm matriz de covariâncias dada por $\mathbf{\Sigma} = (\sigma_{ik})$, mas medidas provenientes
de observações diferentes são não correlacionadas.



## Modelos de Regressão Linear Multivariada


:::: {.columns}

::: {.column width="10%"}
<span style='font-size:100px;'>&#129300;</span> 
:::

::: {.column width="90%"}
<p align="center" class="texto-personalizado">
$\mathcal{B}$ e $\mathbf{\Sigma}$ são desconhecidos...
</p>
:::

::::

Observe que a *i*-ésima coluna da matriz resposta segue o modelo linear univariado dado por:

\vspace{0.3cm}

$$\mathbf{Y}_{(i)} = \mathbf{X} \mathbf{\beta}_{(i)} + \mathbf{\epsilon}_{(i)}, \hspace{0.5cm} i = 1, 2, \cdots, p$$



com $\text{Cov}(\mathbf{\epsilon}_{(i)}) = \sigma_{ii} \mathbf{I}_n$.



## Estimação de Mínimos Quadrados


De acordo com o caso univariado, o estimador de mínimos quadrados para o vetor $\mathbf{\beta}$:


$$\widehat{\mathbf{\beta}}_{(i)} = (\mathbf{X}^t \mathbf{X})^{-1}\mathbf{X}^t \mathbf{Y}_{(i)}, \hspace{0.5cm} i = 1, 2, \cdots, p$$


. . .


Uma vez que $\mathcal{B} = \left[\mathbf{\beta}_{(1)} | \mathbf{\beta}_{(2)} | \cdots | \mathbf{\beta}_{(p)} \right]$, temos

\vspace{0.3cm}

$$\widehat{\mathcal{B}} = \left[\widehat{\mathbf{\beta}}_{(1)} | \widehat{\mathbf{\beta}}_{(2)} | \cdots | \widehat{\mathbf{\beta}}_{(p)} \right] 
= (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \left[\mathbf{Y}_{(1)} | \mathbf{Y}_{(2)} | \cdots | \mathbf{Y}_{(p)} \right]$$

ou,

\vspace{0.3cm}


$$\widehat{\mathcal{B}} = (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{Y}$$

é o estimador de mínimos quadrados da matriz $\mathcal{B}$



## Estimação de Mínimos Quadrados


- Matriz de soma de quadrados e produtos cruzados do erros:

$$\text{SQP Res} = \mathbf{\epsilon}^t\mathbf{\epsilon} = (\mathbf{Y} - \mathbf{X} \widehat{\mathcal{B}})^t(\mathbf{Y} - \mathbf{X} \widehat{\mathcal{B}})$$


. . .


- Valores ajustados:

$$\widehat{\mathbf{Y}} = \mathbf{X} \widehat{\mathcal{B}} = \mathbf{X}(\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{Y}$$



. . .



- Resíduos:

$$\widehat{\mathbf{\epsilon}} = (\mathbf{Y} - \widehat{\mathbf{Y}}) =  (\mathbf{Y} - \mathbf{X}\widehat{\mathcal{B}}) = [\mathbf{I} - \mathbf{X}( \mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t] \mathbf{Y}$$
 
 
## Propriedades

<br>

Condições de ortogonalidade...

$$\mathbf{X}^t \widehat{\mathbf{\epsilon}} = \mathbf{X}^t[\mathbf{I} - \mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t] \mathbf{Y} =  \mathbf{0}$$

$$ \widehat{\mathbf{Y}}^t \widehat{\mathbf{\epsilon}} = \widehat{\mathcal{B}}^t  \mathbf{X}^t[\mathbf{I} - \mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t] \mathbf{Y} = \mathbf{0}$$

. . .


Uma vez que $\mathbf{Y} = \widehat{\mathbf{Y}} + \widehat{\mathbf{\epsilon}}$,

$$\mathbf{Y}^t\mathbf{Y} = (\widehat{\mathbf{Y}} + \widehat{\mathbf{\epsilon}})^t(\widehat{\mathbf{Y}} + \widehat{\mathbf{\epsilon}}) = 
\widehat{\mathbf{Y}}^t\widehat{\mathbf{Y}} + \widehat{\mathbf{\epsilon}}^t\widehat{\mathbf{\epsilon}}^t + \mathbf{0} + \mathbf{0}^t$$



## Propriedades

<br>

ou,

$$\underbrace{{\mathbf{Y}}^t{\mathbf{Y}}}_{\text{SQP total}} = \underbrace{\widehat{\mathbf{Y}}^t\widehat{\mathbf{Y}}}_{\text{SQP regressão}} + \underbrace{\widehat{\mathbf{\epsilon}}^t\widehat{\mathbf{\epsilon}}^t}_{\text{SQP Res}}$$

. . .


De forma que, a soma de quadrados e produtos cruzados dos resíduos pode ser reescrita como:


$$\widehat{\mathbf{\epsilon}}^t\widehat{\mathbf{\epsilon}}^t = \mathbf{Y}^t\mathbf{Y} - \widehat{\mathbf{Y}}^t\widehat{\mathbf{Y}} = 
\mathbf{Y}^t\mathbf{Y} - \widehat{\mathcal{B}}^t \mathbf{X}^t\mathbf{X}\widehat{\mathcal{B}}$$



## Propriedades do estimador $\widehat{\mathcal{B}}$


Para o estimador de mínimos quadrados $\widehat{\mathcal{B}}$ com a matriz $\mathbf{X}$ de posto completo, tem-se:

$$
\begin{eqnarray*}
 E(\widehat{\mathcal{B}}) &=& E[(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\mathbf{Y}] = (\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^tE(\mathbf{Y}) = \\ &=& (\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\mathbf{X}\mathcal{B} = \mathbf{I}\mathcal{B} = \mathcal{B}
\end{eqnarray*}
$$

. . .


Além disso,

$$\text{Cov}(\widehat{\mathbf{\beta}}_{(i)},\widehat{\mathbf{\beta}}_{(k)}) = \sigma_{ik}(\mathbf{X}^t\mathbf{X})^{-1}, \hspace{0.5cm} 
E(\widehat{\mathbf{\epsilon}}) = \mathbf{0}, \hspace{0.5cm} E \left(\displaystyle{\dfrac{\widehat{\mathbf{\epsilon}}^t 
\widehat{\mathbf{\epsilon}}}{n - r - 1 }} \right) = \mathbf{\Sigma} $$


## Propriedades do estimador $\widehat{\mathcal{B}}$

<br>


- $\widehat{\mathcal{B}}$ e $\widehat{\mathbf{\epsilon}}$ são não correlacionados.

. . .

- Estimador não viciado para $\mathbf{\Sigma}$:

$$\widehat{\mathbf{\Sigma}} = \mathbf{S} = \displaystyle{\dfrac{\widehat{\mathbf{\epsilon}}^t\widehat{\mathbf{\epsilon}}}{n - r - 1}} = 
\displaystyle{\dfrac{(\mathbf{Y} - \mathbf{X} \widehat{\mathcal{B}})^t(\mathbf{Y} - \mathbf{X} \widehat{\mathcal{B}})}{n - r - 1}} = 
\displaystyle{\dfrac{\mathbf {Y}^t\mathbf{Y} - \widehat{\mathcal{B}}^t \mathbf{X}^t\mathbf{X}\widehat{\mathcal{B}}}{n - r - 1}}$$



## Exemplo: O impacto do estresse materno no baixo peso ao nascimento


<br>

Os dados do arquivo **Exemplo_regressao_02.dat** referem-se à medidas antropométricas, sócio-econômicas e variáveis relacionadas
ao nível de estresse de 50 gestantes de um determinado município, mensuradas no último trimestre de gestação. 

O estresse materno foi avaliado através de quatro variáveis distintas: resultado do teste de Estado de Ansiedade (EA), resultado
do teste de Traço de Ansiedade (TA), resultado do Questionário Geral de Saúde (QGS) e Escala de Percepção de Estresse (EPE).



## Exemplo: O impacto do estresse materno no baixo peso ao nascimento

<br>


- Como **variáveis resposta**, foram avaliados: peso da criança ao nascer (PESO), medido em gramas e a idade gestacional do recém-nascido,
(IG), medida em semanas. 



- As **variáveis explicativas ou independentes** foram: peso materno (PESOM), em kg, altura materna (ALTURAM), em metros, idade (IDADEM), em
anos, renda *per capta* (RENDA), além dos resultados dos testes de ansiedade. 


## Exemplo: O impacto do estresse materno no baixo peso ao nascimento


```{r, eval=TRUE}
library(pacman)
pacman::p_load(
  tidyverse,
  ggResidpanel
)
```

```{r}
dados = read.table("https://raw.githubusercontent.com/tiagomartin/est022/refs/heads/main/dados/Exemplo_regressao_02.dat", header = TRUE) 
dados %>% str()
```


## Exemplo: O impacto do estresse materno no baixo peso ao nascimento


```{r}
#| output-location: column
attach(dados)

n <- length(PESO)
n
```


```{r}
#| output-location: column

## Matriz Y
Y = cbind(PESO,IG)
Y %>% head()
dim(Y)
```




```{r}
#| output-location: column

## Matriz X
X0 <- rep(1,n)
X = cbind(X0,PESOM,ALTURAM,IDADEM,RENDA,EA,TA,QGS,EPE)
X %>% head()
dim(X)
```



## Exemplo: O impacto do estresse materno no baixo peso ao nascimento

<br>

```{r}
#| output-location: column
## Estimando a matriz B atraves do metodo de minimos quadrados

XLX <- t(X)%*%X
XLY <- t(X)%*%Y

B <- solve(XLX)%*%XLY
B
```




## Exemplo: O impacto do estresse materno no baixo peso ao nascimento

<br>

```{r}
#| output-location: column
## Matriz de soma de quadrados e produtos do residuo

SQPRes = t(Y - X%*%B)%*%(Y - X%*%B)
SQPRes
```

```{r}
#| output-location: column
## Valores ajustados

Ych = X%*%B
Ych %>% head()
```


## Exemplo: O impacto do estresse materno no baixo peso ao nascimento

<br>

```{r}
#| output-location: column
## Residuos

res = Y - Ych
res %>% head()
```


```{r}
#| output-location: column
## Residuos

plot(res)
abline(h=0, col = "red", lty = 3,lwd = 5)
```


## Exemplo: O impacto do estresse materno no baixo peso ao nascimento

<br>


```{r}
#| output-location: column
## Condicoes de ortogonalidade

t(X)%*%res
```

```{r}
#| output-location: column
t(Ych)%*%res
```



## Exemplo: O impacto do estresse materno no baixo peso ao nascimento

<br>

```{r}
#| output-location: column
## Soma de quadrados e produtos total

SQPTot = t(Y)%*%Y
SQPTot
```

```{r}
#| output-location: column
## Soma de quadrados e produtos da regressao

SQPReg = t(Ych)%*%Ych
SQPReg
```




## Exemplo: O impacto do estresse materno no baixo peso ao nascimento

<br>

```{r}
#| output-location: column
## Soma de quadrados e produtos da regressao
Y = cbind(PESO,IG)
X = cbind(PESOM,ALTURAM,IDADEM,RENDA,EA,TA,QGS,EPE)

model = lm(Y~X)
summary(model)
```


## Exemplo: O impacto do estresse materno no baixo peso ao nascimento

```{r}
model01 = lm(PESO ~ PESOM+ALTURAM+IDADEM+RENDA+EA+TA+QGS+EPE)
summary(model01)
```

## Exemplo: O impacto do estresse materno no baixo peso ao nascimento

```{r}
model02 = lm(IG ~ PESOM+ALTURAM+IDADEM+RENDA+EA+TA+QGS+EPE)
summary(model02)
```

```{r}
detach(dados)
```


## Exemplo: O impacto do estresse materno no baixo peso ao nascimento



```{r}
resid_panel(model01)
```



## Exemplo: O impacto do estresse materno no baixo peso ao nascimento



```{r}
resid_panel(model02)
```
